# RL-programming-assg-1

---

# ðŸš€ Contextual Bandits for Server Allocation (CS4122 - RL Assignment 1)

This repository contains the implementation and experiments for our **Reinforcement Learning (RL) Programming Assignment 1**. The task was to solve a **Contextual Bandit problem** using three different strategies:  
- **Îµ-Greedy**  
- **Upper Confidence Bound (UCB)**  
- **Policy Gradient**

The environment is a custom Gymnasium-based simulator: `ServerAllocationEnv`, representing a data center job scheduling problem.

---

## ðŸ“ Files in the Repository

| File | Description |
|------|-------------|
| `lin_greedy.py` | Îµ-Greedy implementation |
| `lin_ucb.py` | Upper Confidence Bound (UCB) implementation |
| `policy_gradient.py` | Policy Gradient algorithm |
| `CloudComputing.py` | Provided environment file (do not modify) |
| `report.pdf` | Final report detailing the analysis, code, and results |
| `README.md` | You are here! |

---

## ðŸ§  Problem Summary

The **Server Allocation Environment** simulates jobs arriving at a data center. Each job has features like:
- Priority
- Job Type (A, B, C)
- Network Usage
- Estimated Processing Time

The **goal** is to allocate the optimal number of servers to each batch of jobs, minimizing cost (or equivalently, maximizing reward).

### ðŸ¤– Why Contextual Bandits?

This environment is a **stochastic contextual bandit** setup due to:
- Random number of jobs per time step
- Variable job characteristics
- Noisy rewards based on latency and energy consumption

---

## ðŸ› ï¸ Feature Engineering

- States (contexts) are extracted by **averaging job features** into a **fixed-size 5D vector**.
- Features: `avg_priority`, `avg_job_type`, `avg_network`, `avg_processing_time`, `job_count`.

Normalization is performed using **Welfordâ€™s incremental mean-variance algorithm**.

---

## ðŸ“ˆ Methods Implemented

### 1. Îµ-Greedy (`lin_greedy.py`)
- Explores with probability Îµ, otherwise selects the best-known action.

### 2. UCB (`lin_ucb.py`)
- Balances exploration and exploitation using upper confidence bounds.
- Evaluation showed correct correlations between:
  - Priority and servers allocated
  - Processing time and servers allocated
  - Number of jobs and servers allocated

### 3. Policy Gradient (`policy_gradient.py`)
- Stochastic policy trained via gradient ascent
- Softmax output over 8 discrete actions
- Exploration managed by temperature scheduling (decayed over batches)

---

## ðŸ“Š Results

### âœ… UCB Logical Evaluation:
- More servers for higher priority or longer jobs.
- Dynamic adjustment based on number of jobs.

### âœ… Policy Gradient:
- Learns through reward-weighted updates.
- Final rewards plotted as **sliding-window averaged graphs**.

---

## ðŸ“Œ How to Run

```bash
# Install dependencies
pip install tensorflow gymnasium matplotlib numpy

# Run Policy Gradient
python policy_gradient.py

# Run UCB
python lin_ucb.py

# Run Îµ-Greedy
python lin_greedy.py
```

> Make sure `CloudComputing.py` is present as it contains the environment logic.

---

## ðŸ§ª Evaluation Metrics

- **Average reward** over batches
- **Receding window plots** for trend visualization
- **Logical consistency** in server allocation behavior
